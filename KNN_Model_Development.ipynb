{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9982d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Columns found: ['date', 'customer_id', 'location', 'business_sector', 'age', 'income', 'credit_score', 'savings_ratio', 'loan_amount', 'debt_to_income', 'credit_utilization', 'payment_punctuality', 'customer_feedback', 'default_history', 'sentiment', 'risk_category', 'customer_segment', 'feedback_topic']\n",
      "Training shape: (1200, 14)\n",
      "Testing shape: (300, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 1. Load the Dataset\n",
    "try:\n",
    "    try:\n",
    "        df = pd.read_csv('data/2025_Sterling_Financial_Dataset_clean.csv')\n",
    "    except FileNotFoundError:\n",
    "        df = pd.read_csv('2025_Sterling_Financial_Dataset_clean.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Columns found:\", df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please upload '2025_Sterling_Financial_Dataset_clean.csv'\")\n",
    "\n",
    "# 2. Define X (Features) and y (Target)\n",
    "target = 'default_history'\n",
    "\n",
    "# Columns to drop (ID, Date, and raw Text are not useful for KNN)\n",
    "# Kept 'sentiment' so that can drop the raw 'customer_feedback'\n",
    "cols_to_drop = [target, 'customer_id', 'date', 'customer_feedback']\n",
    "\n",
    "# Only drop columns that actually exist to avoid KeyError\n",
    "existing_drop_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "X = df.drop(columns=existing_drop_cols)\n",
    "y = df[target]\n",
    "\n",
    "# 3. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training shape: {X_train.shape}\")\n",
    "print(f\"Testing shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472f375",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d627c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 1. Identify which columns are numbers vs text\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 2. Define the Transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 3. Create the Main Pipeline\n",
    "knn_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', RandomUnderSampler(random_state=42)),  # Balanced the classes by removing some '0's\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bb044",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a54e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search...\n",
      "Search Complete!\n",
      "Best k (neighbors): 15\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the options to test\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# 2. Run the Search\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring='f1', n_jobs=None)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 3. Store the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Search Complete!\")\n",
    "print(f\"Best k (neighbors): {grid_search.best_params_['knn__n_neighbors']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
